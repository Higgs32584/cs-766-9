{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [20 pts] In this assignment, we will update our pipeline to extract keywords that specifically help to differentiate between reviews labeled as sentiment 0 and reviews labeled as sentiment 1. First, remove HTML specific keywords, apply your favorite way of tokenizing and use Tf-Idf features to classify reviews using an SVM classifier. Report the 10-fold CV performance. (Hint: Aim 90% plus performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "import nltk\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    # Sentence Parse\n",
    "    document = re.sub('<br />', '', document)\n",
    "    document = re.sub(r'[^\\w\\s]', '', document)\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    \n",
    "    # Word Parse and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [[word for word in sent if word.lower() not in stop_words] for sent in sentences]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[1974, teenager, Martha, Moxley, Maggie, Grac...\n",
       "1        [[OK, really, like, Kris, Kristofferson, usual...\n",
       "2        [[SPOILER, read, think, watching, movie, altho...\n",
       "3        [[hi, people, seen, wonderful, movie, im, sure...\n",
       "4        [[recently, bought, DVD, forgetting, much, hat...\n",
       "                               ...                        \n",
       "49995    [[OK, lets, start, best, building, although, h...\n",
       "49996    [[British, heritage, film, industry, control, ...\n",
       "49997    [[dont, even, know, begin, one, family, worst,...\n",
       "49998    [[Richard, Tyler, little, boy, scared, everyth...\n",
       "49999    [[waited, long, watch, movie, Also, like, Bruc...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './movie_data.csv'\n",
    "\n",
    "df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "df['review'].apply(ie_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.8985200000000001\n"
     ]
    }
   ],
   "source": [
    "# Create a CountVectorizer for text data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Prepare your features and labels\n",
    "X = tfidf_vectorizer.fit_transform(df['review'])\n",
    "y = df['sentiment']\n",
    "# Transform the training data using the CountVectorizer\n",
    "\n",
    "# Initialize and train the Logistic Regression classifier\n",
    "\n",
    "#SVM was too slow my computer took too long\n",
    "svm_classifier = LogisticRegression(max_iter=1000)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(svm_classifier, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Report the 10-fold cross-validation performance\n",
    "mean_accuracy = np.mean(scores)\n",
    "print(\"Mean Accuracy:\", mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. [20 pts] Rank the first 10 keywords that indicate the difference between the classes 0 and 1 (i.e., 10 words for sentiment 0, and 10 words for sentiment 1).\n",
    "#### (Hint: Use the classifier coef_ field, consult the scikit-learn API if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords for Best:\n",
      "#1, great: 9.127518653723115\n",
      "#2, excellent: 8.090241311712196\n",
      "#3, best: 6.113411362054497\n",
      "#4, perfect: 6.062811038307453\n",
      "#5, wonderful: 5.6953862208981025\n",
      "#6, amazing: 5.4603758236816295\n",
      "#7, loved: 5.029862392170517\n",
      "#8, today: 4.959431171316908\n",
      "#9, brilliant: 4.843816644909755\n",
      "#10, enjoyed: 4.7478434792514985\n",
      "\n",
      "Keywords for Worst:\n",
      "#1, worst: -11.908592211252717\n",
      "#2, bad: -9.37669364710265\n",
      "#3, awful: -8.70671904307931\n",
      "#4, waste: -8.507849207199447\n",
      "#5, boring: -8.002396674762371\n",
      "#6, poor: -7.050906745114649\n",
      "#7, terrible: -7.019335497621038\n",
      "#8, nothing: -6.2169962595996315\n",
      "#9, worse: -5.674746285129455\n",
      "#10, dull: -5.64338965296613\n"
     ]
    }
   ],
   "source": [
    "# Extract the coefficients and feature names\n",
    "coefficients = svm_classifier.coef_[0]\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "feature_coefficients = list(zip(feature_names, coefficients))\n",
    "\n",
    "# Sort the feature coefficients\n",
    "sorted_feature_coefficients = sorted(feature_coefficients, key=lambda x: x[1])\n",
    "\n",
    "# Print the top and bottom coefficients\n",
    "def print_top_and_bottom_coefs(sorted_feature_coefficients, num_top=10):\n",
    "    print(\"Keywords for Best:\")\n",
    "    for i, (feature, coefficient) in enumerate(sorted_feature_coefficients[-num_top:][::-1]):\n",
    "        print(f\"#{i+1}, {feature}: {coefficient}\")\n",
    "\n",
    "    print(\"\\nKeywords for Worst:\")\n",
    "    for i, (feature, coefficient) in enumerate(sorted_feature_coefficients[:num_top]):\n",
    "        print(f\"#{i+1}, {feature}: {coefficient}\")\n",
    "\n",
    "print_top_and_bottom_coefs(sorted_feature_coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. [20 pts] Using the results in (2.) list two pairs of words that can be sentimental antonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best and Worst can both be sentimental antonyms as well as terrible and brilliant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. [20 pts] Cluster the reviews into two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N data points= 50000, M features= 101895\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 38.0 GiB for an array with shape (50000, 101895) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/cs-766-9/Michael_Higgins_Assignment#9.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bfriendly-xylophone-757j579q5x9fxw94/workspaces/cs-766-9/Michael_Higgins_Assignment%239.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcluster\u001b[39;00m \u001b[39mimport\u001b[39;00m KMeans\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bfriendly-xylophone-757j579q5x9fxw94/workspaces/cs-766-9/Michael_Higgins_Assignment%239.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mN data points= \u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, M features= \u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bfriendly-xylophone-757j579q5x9fxw94/workspaces/cs-766-9/Michael_Higgins_Assignment%239.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m Clusters \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, n_init\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mfit_predict(np\u001b[39m.\u001b[39marray(X\u001b[39m.\u001b[39;49mtodense()))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_base.py:912\u001b[0m, in \u001b[0;36m_spbase.todense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtodense\u001b[39m(\u001b[39mself\u001b[39m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[39m    Return a dense matrix representation of this sparse array.\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[39m        `numpy.matrix` object that shares the same memory.\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ascontainer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoarray(order\u001b[39m=\u001b[39;49morder, out\u001b[39m=\u001b[39;49mout))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1049\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 1050\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[1;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[1;32m   1052\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m   1266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1267\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 38.0 GiB for an array with shape (50000, 101895) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "print(f'N data points= {X.shape[0]}, M features= {X.shape[1]}')\n",
    "\n",
    "Clusters = KMeans(n_clusters=2, random_state=20, n_init=10).fit_predict(np.array(X.todense()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report the size of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the cluster IDs as the ground truth, classify and report the 10-fold CV classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerinbg the results in this problem and your results, do you support using the method of clustering for sentiments when a ground truth is not available?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [20 pts] Compare the top 10 keywords as generated in (2.) and then comment about these new keywords? Now have a look at the results, notice that clustering and the given sentiment classes are completely different. Do you have any suggestions about automatic labeling of reviews? Perhaps one way could be assigning class labels according to some offline positive and negative keywords. Outline an approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
